\documentclass{scrartcl}

\title{A1 - HMM}
\subtitle{Artificial Intelligence}
\author{Franco Ruggeri}
\date{December 18, 2019}
\setlength{\parindent}{0pt}

\usepackage{amsmath}
\usepackage[backend=biber]{biblatex}
\usepackage{color,soul}

\bibliography{bibliography.bib}

\DeclareMathOperator*{\argmax}{arg\,max}


\begin{document}
\maketitle

\section{Grade E-D}

\subsection{Question 1}

\begin{equation}
 \mathbf{A} = 
 \begin{bmatrix}
  0.5 & 0.5 \\
  0.5 & 0.5
 \end{bmatrix} 
 ,\ \ \mathbf{B} = 
 \begin{bmatrix}
  0.1 & 0.9 \\
  0.5 & 0.5
 \end{bmatrix} 
 ,\ \ \pi = 
 \begin{bmatrix}
  0.5 & 0.5
 \end{bmatrix} 
\end{equation}

\subsection{Question 2}

\begin{equation}
 P(X_6)\ \mathbf{A} = P(X_7)
\end{equation}
 
\subsection{Question 3}

\begin{equation}
 P(X_7)\ \mathbf{B} = P(O_7)
\end{equation}

\subsection{Question 4}

\begin{align}
 P(O_{1:t}=o_{1:t}, X_t=x_i) &= P(O_t=o_t, O_{1:t-1}=o_{1:t-1}, X_t=x_i) \nonumber \\
 &= \{product\ rule\} \nonumber \\
 &= P(O_t=o_t | X_t=x_i, O_{1:t-1}=o_{1:t-1})\ P(X_t=x_i, O_{1:t-1}=o_{1:t-1}) \nonumber \\
 &= \{conditional\ independence\} \nonumber \\
 &= P(O_t=o_t | X_t=x_i)\ P(X_t=x_i, O_{1:t-1}=o_{1:t-1})
\end{align}

\subsection{Question 5}

\begin{itemize}
 \item $\delta$ has $TxN$ elements
 \item $\delta^idx$ has $(T-1)xN$ elements (no predecessor for $t=0$)
\end{itemize}

\subsection{Question 6}

\begin{align} \label{eq:q6}
 P(X_t=x_i, X_{t+1}=x_j | O_{1:T}=o_{1:T}) &= \{definition\ of\ conditional\ probability\} \nonumber \\
 &= \frac{P(X_t=x_i, X_{t+1}=x_j, O_{1:T}=o_{1:T})}{P(O_{1:T}=o_{1:T})}
\end{align}
The denominator of (\ref{eq:q6}) can be computed using the forward algorithm as $\sum_{k=1}^N \alpha_T(k)$. This term represents a normalization factor.


\section{Grade C}

\subsection{Question 7}

According to \cite{rabiner1989tutorial}, a possible distance measure between two HMMs is:
\begin{equation}
 D(\lambda_1, \lambda_2) = \frac{1}{T} \left[ \log P(O_{1:T}|\lambda_1) - \log P(O_{1:T}|\lambda_2) \right]
\end{equation}
It can be used to define the convergence of the algorithm; that is, if the distance between the result of Baum-Welch algorithm and the generating HMM is little, the algorithm has converged. \\

With 1000 observations:
\begin{equation}
 \mathbf{A} = 
 \begin{bmatrix}
  0.7 & 0.1 & 0.29 \\
  0.1 & 0.81 & 0.09 \\
  0.19 & 0.3 & 0.51
 \end{bmatrix} 
 ,\ \ \mathbf{B} = 
 \begin{bmatrix}
  0.69 & 0.23 & 0.08 & 0.01 \\
  0.07 & 0.41 & 0.28 & 0.24 \\
  0 & 0 & 0.35 & 0.65
 \end{bmatrix}
 ,\ \ D(\lambda_1, \lambda_2) \approx 0.00592
\end{equation}

With 10000 observations:
\begin{equation}
 \mathbf{A} = 
 \begin{bmatrix}
  0.69 & 0.04 & 0.26 \\
  0.12 & 0.75 & 0.14 \\
  0.15 & 0.26 & 0.59
 \end{bmatrix}
 ,\ \ \mathbf{B} = 
 \begin{bmatrix}
  0.71 & 0.19 & 0.1 & 0 \\
  0.1 & 0.42 & 0.31 & 0.17 \\
  0.03 & 0.17 & 0.19 & 0.61
 \end{bmatrix}
 ,\ \ D(\lambda_1, \lambda_2) \approx 0.00079
\end{equation}
As we can see, more observations give a better convergence.

\subsection{Question 8}

Assuming no prior knowledge, a good initialization is:
\begin{equation}
 a_{ij} \approx 1/N,\ \ b_{ij} \approx 1/K,\ \ \pi_i \approx 1/N
\end{equation}
Because, being in the middle, it is more probable to reach the global maximum instead of getting stuck in a local one. \\

So, using this initialization the result after learning with 1000 observations is:
\begin{equation}
  \mathbf{A} = 
 \begin{bmatrix}
  0.7 & 0.29 & 0.01 \\
  0.19 & 0.51 & 0.3 \\
  0.1 & 0.09 & 0.81
 \end{bmatrix} 
 ,\ \ \mathbf{B} = 
 \begin{bmatrix}
  0.69 & 0.23 & 0.08 & 0.01 \\
  0 & 0 & 0.35 & 0.65 \\
  0.07 & 0.41 & 0.28 & 0.24
 \end{bmatrix} 
 ,\ \ D(\lambda_1, \lambda_2) \approx 0.00592
\end{equation}
that is exactly the same obtained in question 7 with the initialization close to the generating model. \\

We can notice that the states 2 and 3 are swapped (i.e. $X_2$ of the learned model is $X_3$ of the generating one and vice versa). This is a problem if we want to compute an element-by-element distance (e.g. Euclidean distance), but if we use the distance measure defined in question 7 it does not matter.

\subsection{Question 9}



\begin{center}
 \begin{tabular}{||c c c||} 
 \hline
 T & N & $|D(\lambda_1, \lambda_2)|$ \\ [0.5ex] 
 \hline\hline
 10000 & 5 & 0.0014 \\ 
 \hline
 10000 & 4 & 0.0012 \\
 \hline
 \hl{10000} & \hl{3} & \hl{0.0008} \\
 \hline
 10000 & 2 & 0.0044 \\
 \hline
 10000 & 1 & 0.03949 \\
 \hline
 \hline
 1000 & 5 & 0.0135 \\ 
 \hline
 1000 & 4 & 0.0111 \\
 \hline
 1000 & 3 & 0.0059 \\
 \hline
 \hl{1000} & \hl{2} & \hl{0.0032} \\
 \hline
 1000 & 1 & 0.0418 \\
 \hline
 \hline
 10 & 5 & 0.9030 \\ 
 \hline
 10 & 4 & 0.6629 \\
 \hline
 10 & 3 & 0.5072 \\
 \hline
 10 & 2 & 0.1470 \\
 \hline
 \hl{10} & \hl{1} & \hl{0.0391} \\ [1ex] 
 \hline
\end{tabular}
\end{center}

As we can see, if enough data are available, the best number of hidden states is of course 3 (same as generating model). However, in case of few data, this is not true: with 1000 and 100 observations the best number of hidden states is 2 and 1, respectively. This confirms that more hidden states ($\rightarrow$ more parameters) require more data, as the Baum-Welch algorithm is based on statistics and there has to be enough data for those to be significant.

\subsection{Question 10}

Initializing with (exact) uniform distribution, the Baum-Welch algorithm produces:
\begin{equation}
  \mathbf{A} = 
 \begin{bmatrix}
  0.33 & 0.33 & 0.33 \\
  0.33 & 0.33 & 0.33 \\
  0.33 & 0.33 & 0.33 \\
 \end{bmatrix} 
 ,\ \ \mathbf{B} = 
 \begin{bmatrix}
  0.24 & 0.25 & 0.24 & 0.27 \\
  0.24 & 0.25 & 0.24 & 0.27 \\
  0.24 & 0.25 & 0.24 & 0.27 \\
 \end{bmatrix}
\end{equation}
As you can see, the algorithm gets stuck in the initial point, that is a local maximum. Indeed, if the initial $\mathbf{B}$ is uniform, the observations give no information. \\

Initializing with a diagonal $\mathbf{A}$ matrix and $\pi = \left[0,0,1\right]$, we get:
\begin{equation}
  \mathbf{A} = 
 \begin{bmatrix}
  NaN & NaN & NaN \\
  NaN & NaN & NaN \\
  NaN & NaN & NaN \\
 \end{bmatrix} 
 ,\ \ \mathbf{B} = 
 \begin{bmatrix}
  NaN & NaN & NaN & NaN \\
  NaN & NaN & NaN & NaN \\
  NaN & NaN & NaN & NaN \\
 \end{bmatrix}
\end{equation}
The $NaN$s (not a number) come from divisions 0/0. Indeed, the model is stuck into the state 3 ($i=2$) and so $\gamma_t(i,j)=0$ and $\gamma_t(i)=0$ for $i\neq2$.

Finally, initializing the matrices close to the solution guarantees a good convergence, since the global maximum is reached.


\section{Grade B-A}

\subsection{Shooting}

The most likely next move for one single bird is computed as:
\begin{align}
 next\ move &= \argmax_{m} P(O_{T+1}=o_m | O_{1:T}) \nonumber \\
 &= \sum_{i=1}^N P(O_{T+1} | X_{T+1}=x_i)\ P(X_{T+1}=x_i | O_{1:T}) \nonumber \\
 &= \sum_{i=1}^N b_i(O_{T+1})\ P(X_{T+1}=x_i | O_{1:T}) \nonumber \\
\end{align}

Let's compute $P(X_{T+1}=x_i | O_{1:T})$:
\begin{align}
 P(X_{T+1}=x_i | O_{1:T}) &= \sum_{j=1}^N P(X_{T+1}=x_i | X_T=x_j)\ P(X_T=x_j | O_{1:T}) \nonumber \\
 &= \sum_{j=1}^N a_{ji}\ \frac{P(X_T=x_j, O_{1:T})}{P(O_1:T)} \nonumber \\
 &= \sum_{j=1}^N a_{ji}\ \frac{\alpha_T(j)}{\sum_{k=1}^N \alpha_T(k)} \\
 &= \sum_{j=1}^N a_{ji}\ \hat{\alpha}_T(j)
\end{align}

So the complete formula is:
\begin{equation}
 next\ move = \argmax_{m} \sum_{i=1}^N b_i(O_{T+1})\ \sum_{j=1}^N a_{ji}\ \hat{\alpha}_T(j)
\end{equation}

The idea is to use not only the model of the current bird, but also the models of all the birds belonging to the guessed species, which would have to behave like the current one. So, I pick the most likely next move overall. \\

Improvements:
\begin{itemize}
 \item Do not shoot if the guess is unknown or black stork.
 \item Shoot only if $confidence > threshold$ (set to 0.75).
\end{itemize}

\subsection{Guessing}
As for shooting, the idea is to use all the available models of the birds whose species has been revealed. So, I pick the species of the bird whose model maximizes the likelihood of the observation sequence (i.e. evaluation problem solved with $\alpha$-pass, species recognition). \\

Improvements:
\begin{itemize}
 \item First round: guess randomly to get information (actually it is better to guess always one species).
 \item Next rounds: guess randomly when the recognition fails (unknown result) to get information.
\end{itemize}


\clearpage
\newpage
\printbibliography

\end{document}

